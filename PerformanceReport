#https://towardsdatascience.com/sentiment-analysis-of-stocks-from-financial-news-using-python-82ebdcefb638
# Import libraries
from urllib.request import urlopen, Request
from bs4 import BeautifulSoup
import os
import datetime
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
# NLTK VADER for sentiment analysis
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk
nltk.download('vader_lexicon')

finwiz_url = 'https://finviz.com/quote.ashx?t='
#######
news_tables = {}
#tickers = ["GME","SLV", "SPCE"]
tickers = ["XOM","GS", "ETSY"]

for ticker in tickers:
    url = finwiz_url + ticker
    req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) 
    response = urlopen(req)    
    # Read the contents of the file into 'html'
    html = BeautifulSoup(response)
    # Find 'news-table' in the Soup and load it into 'news_table'
    news_table = html.find(id='news-table')
    # Add the table to our dictionary
    news_tables[ticker] = news_table
#######    
    # Read one single day of headlines for 'AMZN' 
amzn = news_tables['XOM']
# Get all the table rows tagged in HTML with <tr> into 'amzn_tr'
amzn_tr = amzn.findAll('tr')

for i, table_row in enumerate(amzn_tr):
    # Read the text of the element 'a' into 'link_text'
    a_text = table_row.a.text
    # Read the text of the element 'td' into 'data_text'
    td_text = table_row.td.text
    # Print the contents of 'link_text' and 'data_text' 
    print(a_text)
    print(td_text)
    # Exit after printing 4 rows of data
    if i == 3:
        break

parsed_news = []
        
#######  

# Iterate through the news
for file_name, news_table in news_tables.items():
    # Iterate through all tr tags in 'news_table'
    for x in news_table.findAll('tr'):
        # read the text from each tr tag into text
        # get text from a only
        text = x.a.get_text() 
        # splite text in the td tag into a list 
        date_scrape = x.td.text.split()
        # if the length of 'date_scrape' is 1, load 'time' as the only element

        if len(date_scrape) == 1:
            time = date_scrape[0]
            
        # else load 'date' as the 1st element and 'time' as the second    
        else:
            date = date_scrape[0]
            time = date_scrape[1]
        # Extract the ticker from the file name, get the string up to the 1st '_'  
        ticker = file_name.split('_')[0]
        
        # Append ticker, date, time and headline as a list to the 'parsed_news' list
        parsed_news.append([ticker, date, time, text])
        
parsed_news

#######  

# Instantiate the sentiment intensity analyzer
vader = SentimentIntensityAnalyzer()

# Set column names
columns = ['ticker', 'date', 'time', 'headline']

# Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'
parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)

# Iterate through the headlines and get the polarity scores using vader
scores = parsed_and_scored_news['headline'].apply(vader.polarity_scores).tolist()

# Convert the 'scores' list of dicts into a DataFrame
scores_df = pd.DataFrame(scores)

# Join the DataFrames of the news and the list of dicts
parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')

# Convert the date column from string to datetime
parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date
savedDF = parsed_and_scored_news.head().to_html()         ############### The dataframe I include in my email
parsed_and_scored_news.head()

####### 
plt.rcParams['figure.figsize'] = [10, 6]

# Group by date and ticker columns from scored_news and calculate the mean
mean_scores = parsed_and_scored_news.groupby(['ticker','date']).mean()

# Unstack the column ticker
mean_scores = mean_scores.unstack()

# Get the cross-section of compound in the 'columns' axis
mean_scores = mean_scores.xs('compound', axis="columns").transpose()

start = datetime.date(2021,1,1)
end = datetime.date(2021,3,26)
mean_scores_filtered = mean_scores.loc[start:end]

# Plot a bar chart with pandas
mean_scores_filtered.plot(kind = 'bar')
plt.grid()
plt.savefig('C:\\Users\\16158\\CWScratch\\Alpaca\\files\\plot')



#####Get AlpacaData
from datetime import datetime
import numpy as np
import alpaca_trade_api as tradeapi
import pandas as pd
import threading
from time import sleep 
import json
import logging
import matplotlib.pyplot as plt


APCA_API_KEY_ID = 'PKC7N32R5TA33ABKOWFI'
APCA_API_SECRET_KEY = 'XrQUKqXhcKBrUP1bGdFJos88acVn2N7PYEJDkLXu'
APCA_API_BASE_URL = 'https://paper-api.alpaca.markets'

api = tradeapi.REST(APCA_API_KEY_ID, APCA_API_SECRET_KEY, APCA_API_BASE_URL, api_version='v2')

######################################## Portfolio History
history=api.get_portfolio_history()

timeList = []
equityList = []
profit_lossList = []
profit_loss_pctList = []

# Reads, formats and stores the new bars
for i in range(len(history.timestamp)):
    timeList.append(datetime.fromtimestamp((history.timestamp[i])))
    equityList.append(history.equity[i])
    profit_lossList.append(history.profit_loss[i])
    profit_loss_pctList.append(history.profit_loss_pct[i])
history = pd.DataFrame(list(zip(timeList,equityList,profit_lossList,profit_loss_pctList)), 
               columns =['time','equity','P&L','P&L%']) 
history 

######################################## Buys Sales
activities=api.get_activities()

timeList = []
qtyList = []
symbolList = []
sideList = []
priceList = []

# Reads, formats and stores the new bars
for i in range(len(activities)):
    timeList.append(datetime.fromisoformat(str((activities[i].transaction_time))))
    #timeList.append(activities[i].transaction_time)
    symbolList.append(activities[i].symbol)
    sideList.append(activities[i].side)
    qtyList.append(activities[i].qty)
    priceList.append(activities[i].price)
buyssales = pd.DataFrame(list(zip(timeList,symbolList,sideList,priceList, qtyList)), 
               columns =['time','symbol','side','price', 'qty']) 
buyssales 

######################################## VIX Table

import yfinance as yf    #https://algotrading101.com/learn/yfinance-guide/
import math
#monthlyvix=str(round(yf.Ticker("^VIX").history(period="0").iloc[0].Close/math.sqrt(12),2))
vixdf = yf.Ticker("^VIX").history(start = (datetime.today() - timedelta(days=365)), end = datetime.today())
impliedList = []
# Reads, formats and stores the new bars
for i in range(len(vixdf)):
    impliedList.append(float(round(vixdf.iloc[i].Close/math.sqrt(12),2)))
vixdf['impliedvixmove']=impliedList

######################################## Sentiment Table

data = yf.download("ETSY GS XOM", start="2021-02-11", end="2021-03-28") ##################Need to udpate this to match mean score dates or it will outer join incompatible date ranges
df2=mean_scores.reset_index()
df3=data.Close.reset_index()
df2['Date'] = df2['date'].astype(str)
df3['Date'] = df3['Date'].astype(str)

sentiment_table = pd.merge(df2, df3, on='Date', how='outer')

Etsy_y_PercList = []
GS_y_PercList = []
XOM_y_PercList = []
# Reads, formats and stores the new bars
for i in range(len(sentiment_table)):
    Etsy_y_PercList.append(sentiment_table.iloc[i].ETSY_y/sentiment_table.iloc[i-1].ETSY_y)
    GS_y_PercList.append(sentiment_table.iloc[i].GS_y/sentiment_table.iloc[i-1].GS_y)
    XOM_y_PercList.append(sentiment_table.iloc[i].XOM_y/sentiment_table.iloc[i-1].XOM_y)
sentiment_table['Etsy_y_%']= Etsy_y_PercList
sentiment_table['GS_y_%']= GS_y_PercList
sentiment_table['XOM_y_%']= XOM_y_PercList

sentiment_table

####################Send Trading performance data to Google Sheets
import gspread
import pandas as pd
from oauth2client.service_account import ServiceAccountCredentials

def iter_pd(df):
    for val in df.columns:
        yield val
    for row in df.to_numpy():
        for val in row:
            if pd.isna(val):
                yield ""
            else:
                yield val

def pandas_to_sheets(pandas_df, sheet, clear = True):
    # Updates all values in a workbook to match a pandas dataframe
    if clear:
        sheet.clear()
    (row, col) = pandas_df.shape
    cells = sheet.range("A1:{}".format(gspread.utils.rowcol_to_a1(row + 1, col)))
    for cell, val in zip(cells, iter_pd(pandas_df)):
        cell.value = val
    sheet.update_cells(cells)

scope = ['https://spreadsheets.google.com/feeds',
         'https://www.googleapis.com/auth/drive']

credentials = ServiceAccountCredentials.from_json_keyfile_name('C:\\Users\\16158\\CWScratch\\Alpaca\\files\\TradingProject-ed0b4f92dbe7.json', scope)
gc = gspread.authorize(credentials)


##Create google sheet
#sh = gc.create('sentiment_table')
#sh.share('firstwestventures@gmail.com', perm_type='user', role='owner')
##send data to gsheet
wks = gc.open("sentiment_table").sheet1
df = sentiment_table
df2=sentiment_table.reset_index()
df2['Date'] = df2['Date'].astype(str) ######converts the Date column to string
df2['date'] = df2['date'].astype(str) ######converts the date column to string ## just if Date is lowercase in orig df
pandas_to_sheets(df2, wks)
